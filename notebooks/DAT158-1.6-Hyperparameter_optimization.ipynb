{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 27.09.2023, A. S. Lundervold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HVL-ML/DAT158/blob/master/notebooks/DAT158-1.6-Hyperparameter_optimization.ipynb)  &nbsp; [![kaggle](https://camo.githubusercontent.com/a08ca511178e691ace596a95d334f73cf4ce06e83a5c4a5169b8bb68cac27bef/68747470733a2f2f6b6167676c652e636f6d2f7374617469632f696d616765732f6f70656e2d696e2d6b6167676c652e737667)](https://www.kaggle.com/alexanderlundervold/2023-dat158-1-5-hyperparameter_optimization-ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from pathlib import Path\n",
    "import seaborn as sns \n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_DIR = Path.cwd()\n",
    "DATA = NB_DIR/'data'\n",
    "DATA.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models typically have several _hyperparameters_ we must choose correctly to achieve higher performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is a hyperparameter?** During training, the model's _parameters_ are automatically tuned to make the model produce useful outputs. This is typically achieved by using an optimization algorithm (for example, gradient descent) to minimize some cost function (for example, mean square error or cross-entropy). \n",
    "\n",
    "> However, there are typically other parameters in the model that are not automatically tuned during training. These are the things you pass to the scikit-learn estimators as parameters. For example `RandomForestClassifier(max_depth = 2, n_estimators = 100, ...)` They are called _hyperparameters_. Examples include things like the learning rate, the amount of regularization, the number of layers in a neural network, and much more. Some models have a large number of such parameters, which can influence their performance heavily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we select good hyperparameters?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's essentially a learning task: train the model to also obtain good hyperparameter settings. However, it's typically not that easy to formulate the task in a way where machine learning training methods can work (it's, for example, challenging to create cost functions for this task that can be optimized using gradient descent since such cost functions wouldn't be differentiable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In other words, **hyperparameters are settings that govern the learning process but are not learned from the data**. They are typically set before training and kept constant during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **There are some exciting methods to make powerful models that optimize ML models, but that is beyond our scope in this notebook**. Have a look at <a href=\"https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html\">AutoML based on reinforcement learning</a> or <a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization\">evolutionary algorithms</a> if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of hyperparameter optimization as tuning a musical instrument: it's about finding the right settings to produce the best sound (or in this case, model performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One approach to hyperparameter selection: simply search!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for good hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two standard ways are \n",
    "1. **brute-force search**: try all parameter combinations within a specified range, and \n",
    "2. **random search**: try out random combinations of parameter settings within a specified range\n",
    "\n",
    "Grid search is exhaustive but computationally expensive, while random search is faster but might miss the optimal setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A third way is to be more clever and use the results obtained from previous parameter settings to select the next setting that is expected to be better. This leads to things like \n",
    "3. **bayesian hyperparameter optimization** \n",
    "\n",
    "and also **evolutionary hyperparameter optimization** (not covered here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One brute force search method often used is **grid search**. In cases where it makes sense to search through a very large space of parameter settings or where you have to do a lot of compute each time you try a setting, it's better to use random search than grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get concrete and try these out on some model trained on some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at a data set of housing prices in various districts in California. To make the story clearer, we'll only tune a few hyperparameters of a random forest model.\n",
    "\n",
    "> Note: as we're focusing on hyperparameter optimization, we'll not spend much time on data exploration and preprocessing. We'll just do the bare minimum to get a model up and running. Feel free to explore the data more and try out different preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://www.dropbox.com/s/quug4svzdyj5k4j/housing_data.csv?dl=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split off some test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('median_house_value', axis=1), \n",
    "                                                    df.median_house_value, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll measure performance using root mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some values are missing in `total_bedrooms`. Let's impute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "features = ['total_bedrooms']\n",
    "imp = SimpleImputer()\n",
    "X_train.loc[:, features] = imp.fit_transform(X_train[features])\n",
    "X_test.loc[:, features] = imp.transform(X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `ocean_proximity` is categorical, not numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to represent this feature in some good way using **feature encoding**. We can, for example, use one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the ocean_proximity column in both the training and testing datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a one-hot encoder object\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the ocean_proximity column in the training dataset\n",
    "ocean_proximity_encoded = ohe.fit_transform(X_train[['ocean_proximity']])\n",
    "ocean_proximity_encoded_df = pd.DataFrame(ocean_proximity_encoded.toarray(), \n",
    "                                          columns=ohe.get_feature_names_out(['ocean_proximity']))\n",
    "X_train = pd.concat([X_train.drop('ocean_proximity', axis=1).reset_index(drop=True), \n",
    "                     ocean_proximity_encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Transform the ocean_proximity column in the testing dataset\n",
    "ocean_proximity_encoded = ohe.transform(X_test[['ocean_proximity']])\n",
    "ocean_proximity_encoded_df = pd.DataFrame(ocean_proximity_encoded.toarray(), \n",
    "                                          columns=ohe.get_feature_names_out(['ocean_proximity']))\n",
    "X_test = pd.concat([X_test.drop('ocean_proximity', axis=1).reset_index(drop=True), \n",
    "                    ocean_proximity_encoded_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling:** It's not necessary to scale the features as we plan to use random forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a model using all the standard hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what the standard settings are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to improve our baseline model by tweaking hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In grid search we select some paramaters to change, make a set or range of values for each of them, and try every combination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify our grid as a Python dictionary, or a list of dictionaries if we want to be more specific about parameter combinations to try (for example, \"if `n_estimators` is 10, try `max_depth` 2, 3 and 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'max_depth': [5, 50, None],\n",
    "    'max_features': [2, 3, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search over all the $5*3*3=45$ different settings, using 3-fold cross validation to check the performance of each one (training the model a total of $45*3 = 135$ times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_reg = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reg = gs_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_best = best_reg.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_best, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try randomly searching through the space of parameters a specified number of iterations. This can be achieved using `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use scikit-learn's `RandomizedSearchCV` in a similar way to `GridSearchCV`: we specify the estimator to use and the parameter grid to search through. But we also specify the number of settings to try, and the method searches randomly through the parameter space that many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_reg = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "                            n_iter=n_iter, cv=3, verbose=1, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only searching through a selection of the settings, this will of course not outperform the brute-force grid search approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test,rs_reg.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data is small it doesn't cost much time to search through *all* the settings in our above parameter grid. That is, using grid search is okay. However, if the parameter space was chosen to be much larger, our data set was more complicated, or our model took much longer to train, random search would be a better approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Your Turn!** Advanced Grid Search and Random Search:\n",
    "\n",
    "1. Extend the parameter grid to include at least two more hyperparameters. Refer to the `RandomForestRegressor` documentation for options.\n",
    "2. Run grid search and random search again with the extended parameter grid.\n",
    "3. Compare the best models from each method. Which one performs better?\n",
    "4. Time how long each method takes to find the best hyperparameters and note it down. What do you observe?Try searching over a larger set of parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced techniques: Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of searching through parameter settings one at a time without paying attention to what's happened in the past, we can use methods for a more intelligent selection of settings to try next. This can significantly reduce the number of iterations necessary to find good parameter settings and can be useful when looking through a large number of options.\n",
    "\n",
    "In Bayesian hyperparameter optimization, you start with an objective function that you want to minimize. It could, for example, be mean squared error. You then want to search through a space of possible hyperparameters in an efficient way without having to try all possibilities and without the trials being completely random. \n",
    "\n",
    "> Think of it as a treasure hunt where each step is informed by all the previous steps, making it more efficient than random or exhaustive searching.\n",
    "\n",
    "The basic idea is to somehow build a _model_ of the objective function (more precisely, a probability distribution for the objective function) and then use the model to select the most promising hyperparameter settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several libraries can achieve this, for example, `hyperopt` https://github.com/hyperopt/hyperopt or `optuna` https://github.com/optuna/optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using ML-techniques to automate the construction of machine learning systems is called **AutoML**, a field of research that has gotten significant attention in recent years (there are also multiple products offering AutoML solutions, e.g. Google's [Cloud AutoML](https://cloud.google.com/automl/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click <font color=\"red\"><b>here</b></font> for a brief introduction to <code>hyperopt</code></summary>\n",
    "  \n",
    "At its core, `hyperopt` uses a probabilistic model to predict which hyperparameter settings are likely to yield lower values of the objective function, in this case, the validation error of a machine learning model.\n",
    "\n",
    "Here's a simplified explanation:\n",
    "\n",
    "1. Objective Function: You define an objective function that takes in hyperparameters and returns a value you want to minimize (or maximize). In machine learning, this is often the validation error of a model trained with those hyperparameters.\n",
    "\n",
    "2. Parameter Space: You also define a space of possible hyperparameters. This can include different types of variables like integers, floats, or categorical variables, and you can specify ranges or sets of values for each.\n",
    "\n",
    "3. Initial Sampling: Initially, Hyperopt randomly samples a few points in the hyperparameter space and evaluates them using the objective function.\n",
    "\n",
    "4. Probabilistic Model: Based on these initial evaluations, Hyperopt constructs a probabilistic model that estimates the objective function across the hyperparameter space.\n",
    "\n",
    "5. Selection of Next Point: Using this model, Hyperopt selects the next hyperparameter setting to evaluate. It chooses the setting that is expected to have the best performance, according to a criterion that balances exploration (trying new parts of the space) and exploitation (refining around the currently best-known settings).\n",
    "\n",
    "6. Update and Iterate: After evaluating the new setting, the probabilistic model is updated, and the process repeats.\n",
    "\n",
    "7. Convergence: This continues until a stopping criterion is met, such as a maximum number of evaluations, or until the model converges to a set of optimal hyperparameters.\n",
    "\n",
    "The algorithm that Hyperopt commonly uses for constructing the probabilistic model and selecting the next point is called the Tree-structured Parzen Estimator (TPE).\n",
    "\n",
    "The TPE algorithm models the objective function as a mixture of two distributions: one for the points where the objective function has a low value (i.e., good hyperparameters) and another for the points where the objective function has a high value (i.e., bad hyperparameters).\n",
    "\n",
    "Here's a simplified explanation:\n",
    "\n",
    "1. Initial Sampling: Like other Bayesian optimization algorithms, TPE starts by evaluating the objective function at a few randomly selected points in the hyperparameter space.\n",
    "\n",
    "2. Divide and Conquer: After each evaluation, TPE divides the observed hyperparameter settings into two groups: those that resulted in good outcomes and those that resulted in bad outcomes, based on a threshold.\n",
    "\n",
    "3. Probabilistic Models: TPE then fits two separate probabilistic models: one for the \"good\" hyperparameters and another for the \"bad\" hyperparameters. These models estimate the likelihood of a given hyperparameter setting being \"good\" or \"bad.\"\n",
    "\n",
    "4. Selection Criterion: To decide the next point to evaluate, TPE calculates the ratio of the likelihoods according to these two models. It selects the point where the ratio of \"good\" to \"bad\" is the highest, meaning it's the most promising point to try next.\n",
    "\n",
    "5. Iterate: The new point is evaluated, the models are updated, and the process repeats.\n",
    "\n",
    "6. Convergence: This continues until a stopping criterion is met, such as a maximum number of evaluations, or the algorithm converges to an optimal set of hyperparameters.\n",
    "\n",
    "In essence, TPE intelligently narrows down the search for optimal hyperparameters by learning from past evaluations. It balances exploration and exploitation by using these probabilistic models to focus more on the regions of the hyperparameter space that are likely to yield better results, while still occasionally trying less likely regions to ensure thoroughness.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<summary>\n",
    "    <b></b>\n",
    "</summary>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that we want to minimize during the optimization process. In our case we want to minimize the mean absolute error of our random forest model, as measured using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(params):\n",
    "    # Our objective is to find the hyperparameters that gives the lowest mean squared error \n",
    "    # on validation data\n",
    "    \n",
    "    print(f\"Using parameters {params}\")\n",
    "    # Our model\n",
    "    model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)\n",
    "    # The max cross-val score with current paramaters:\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"neg_mean_squared_error\").mean()\n",
    "    # We want to minimize the mean absolute error loss, not its negative:\n",
    "    loss = -score\n",
    "    return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select a space of parameters over which to search. We set it up similar to the above `param_grid`, using hyperopt to choose random parameter settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [10, 50, 100, 150, 200]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 50, None]),\n",
    "    'max_features': hp.choice('max_features', [2, 3, None])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually select a random (stochastic) samples from this space, just to check that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt.pyll.stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyperopt.pyll.stochastic.sample(param_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's minimize our objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the trials object we can check what goes on in each trial:\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(fn=objective, space=param_space, algo=tpe.suggest, \n",
    "            max_evals=8, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices for the best parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `space_eval` function to find the corresponding values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_eval(param_space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and train and evaluate a model using those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(**space_eval(param_space, best), random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "mean_squared_error(y_test,model.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Make a function that plots the result of each trial. Hint: use the `trials` object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot the performance of each trial during Bayesian optimization. What trends do you observe? Hint: use the `trials` object.\n",
    "2. Try to search over a much larger set of parameters (and perhaps also over other regression models than `RandomForestRegressor`). How does the performance of Bayesian optimization compare to grid search and random search?\n",
    "2. Try using Bayesian optimization on a different dataset that we've studied earlier in the course.\n",
    "3. Experiment with another machine learning model like SGDRegressor or SVM. How do the optimal hyperparameters differ between models?\n",
    "4. Compare the time efficiency and performance of Bayesian optimization with grid search and random search on this new dataset and model. Which method do you find most effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra exercise\n",
    "Dive into the world of AutoML. Use Google's Cloud AutoML or any other AutoML tool to automatically select the best model and hyperparameters for one of the datasets we've studied. Compare its performance and efficiency with the methods we've manually implemented. What are your observations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT158",
   "language": "python",
   "name": "dat158"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

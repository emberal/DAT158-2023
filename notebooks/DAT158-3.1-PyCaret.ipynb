{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e245b1f",
   "metadata": {},
   "source": [
    "A.S. Lundervold, 15.11.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53799ab3",
   "metadata": {},
   "source": [
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HVL-ML/DAT158/blob/master/notebooks/DAT158-3.1-PyCaret.ipynb)  &nbsp; [![kaggle](https://camo.githubusercontent.com/a08ca511178e691ace596a95d334f73cf4ce06e83a5c4a5169b8bb68cac27bef/68747470733a2f2f6b6167676c652e636f6d2f7374617469632f696d616765732f6f70656e2d696e2d6b6167676c652e737667)](https://www.kaggle.com/alexanderlundervold/2023-dat158-3-1-pycaret-ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe528c8",
   "metadata": {},
   "source": [
    "> **NB**: If you want to run this notebook on your own computer, you must install PyCaret. I recommend creating a new conda environment and running `pip install \"pycaret[full]\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6c866-281e-46c3-8572-90261e51be0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# A quick PyCaret tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0466b1",
   "metadata": {},
   "source": [
    "> PyCaret is an open-source, low-code machine learning library in Python designed to automate and streamline machine learning workflows. It serves as an end-to-end machine learning and model management solution, significantly enhancing productivity and reducing the time needed for experimentation in machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5dd0f7",
   "metadata": {},
   "source": [
    "> This will be a concise tutorial on PyCaret. Consult the documentation at https://pycaret.gitbook.io/docs/ for more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f543164",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3c1e3-d6a4-409f-8c5c-928fce362123",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    %pip install \"pycaret[full]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393defc-2159-4b3b-b2a5-ee5d254b6753",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60035b87",
   "metadata": {},
   "source": [
    "PyCaret comes with functionality for loading many standard benchmark datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc34bf-1560-4511-8d8d-64a285e7d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b609de-cc8d-4978-8353-9082984852a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0a257-58b5-45fe-a1ec-e044fcab9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = get_data('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa6114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20452066-7c75-4e77-9926-3c362ec392d7",
   "metadata": {},
   "source": [
    "> **Your turn!** Later, you should try out some of the other datasets listed above. You can also try to load your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e388693",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cbaa9",
   "metadata": {},
   "source": [
    "We'll use one of the smaller data sets to reduce computational time.  But, of course, you're welcome to try some of the other data sets listed above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed94b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[target].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650e56e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec24f61",
   "metadata": {},
   "source": [
    "After downloading data and looking at its structure, one should start a more thorough exploration. We've seen how this can be done earlier in the course.\n",
    "\n",
    "Here's a convenient package that can perform some of the common exploration steps automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539eaeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611fe42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71ca0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prepare the data and set up an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed471bf-4c9e-4bfb-991f-9740c022f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c021fc2-881d-4f37-9b6e-e35dc3a00c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = setup(data=data, target=target, normalize=True, \n",
    "                   normalize_method='robust', \n",
    "                   log_experiment=True, experiment_name='exp1', \n",
    "                   session_id=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad7ed8",
   "metadata": {},
   "source": [
    "> **Your turn!** Explore the various options PyCaret provides for setting up experiments using `setup`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb14d4-e589-4051-aedb-956cf518a199",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train some baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90f54a-3571-4762-8790-d9d4f9419bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = compare_models(n_select=5, sort='Accuracy', exclude=['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81e9de9-2482-47a7-81df-85b6ea205658",
   "metadata": {},
   "source": [
    "The results from this experiment are saved as a dataframe in our log (later in the notebook, we'll explore the results of our experiments using MLflow): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab3b76-d164-40f3-ae07-6a4f4571017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd8bda-f2f3-4498-8040-09691c6f2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad17a6-2f13-498a-b820-2bae326352d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_logs():\n",
    "    log_df = get_logs()\n",
    "    return log_df.sort_values(by='metrics.Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4c73f-a4c5-424c-b7fe-94002aa0159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sorted_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d0ab4-630d-48fe-988f-b8189932f62b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104f472",
   "metadata": {},
   "source": [
    "We've found some candidate models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88aea1",
   "metadata": {},
   "source": [
    "We'll want to tune their hyperparameters to try to improve their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tuned_models = [tune_model(m, optimize='Accuracy', n_iter=600, fold=5, choose_better=True) \n",
    "                for m in top_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27cd106-bb35-4717-b117-12a0226ba886",
   "metadata": {},
   "source": [
    "**Note:** We've used the default parameter grids set by PyCaret. However, it's often a good idea to investigate more carefully what parameters to consider (as it depends not only on the model but also on the data). You can modify the grid using custom_grid. See the PyCaret source code for the default parameter grids. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d74f7d-9b34-4c7f-913b-72e6889a4004",
   "metadata": {},
   "source": [
    "The logs have now been updated:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12b23a21-fc96-4bbf-a97b-712ce764ed65",
   "metadata": {},
   "source": [
    "get_sorted_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db230b",
   "metadata": {},
   "source": [
    "These scores can be compared to those obtained when we used default parameters. Note that `tune_model` uses `RandomizedSearchCV` from scikit-learn as its default search strategy. It's, therefore, not guaranteed that you will find the best hyperparameter combination from the parameter grid during the search. You can, of course, change the search algorithm and also the search library (for example, you can use `scikit-optimize`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a82a9-1c23-48cd-8e42-2dcb6c228003",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c146f",
   "metadata": {},
   "source": [
    "As we've seen, it's often possible to combine models in a way that outperforms each of the single models. Again, there are multiple ways of doing this. A simple way, as you know, is to use \"voting ensembles.\" In PyCaret, we can use `blend_models` to construct voting ensembles. \n",
    "\n",
    "Let's try ensembling some of the best models found so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac19908f-2ce1-4488-b4dc-63aedca654a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 4\n",
    "best_models = tuned_models[:n_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451a7f3-ab41-4d52-81d4-554452592518",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38562fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_hard = blend_models(best_models, method='hard', optimize='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86fa4a",
   "metadata": {},
   "source": [
    "As we've seen earlier, one can also train a so-called \"blender\" on top of the predictions from a set of models and, in that way, make use of more complicated patterns than in a voting ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1492806-d8d2-42ba-8b62-a5a87734879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blender = stack_models(estimator_list=best_models, optimize='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf788e78-f426-430a-9953-9b9ce8e05179",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Inspect the results of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60151571-d011-46a0-94b5-77d69e83771a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sorted_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783df7e-5ccb-45cd-8922-0846b4a0e394",
   "metadata": {},
   "source": [
    "We can have a look at our results using MLflow (https://mlflow.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77e148-ffa4-4578-a013-315e61f315af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58a8f2-3fb4-43c0-9bd8-faf3f776e361",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d418cd6-6648-4574-ae72-55dff4d262e8",
   "metadata": {},
   "source": [
    "We'll pick the best model trained so far and evaluate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa74f6-de90-4060-b378-4a03f917f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sorted_logs().iloc[0]['artifact_uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c349c2-d5ce-40c6-bcc8-6fce5bdb6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the location of the best model\n",
    "best_model_fn = f\"{get_sorted_logs().iloc[0]['artifact_uri'][7:]}/model/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287a950-a192-4638-b237-99260bb1f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8b9bf-c6e4-4235-b381-e0a61606e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(best_model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0446c500-7cc4-4803-abbf-45ba7848d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753e2e9",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8515fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff4ea9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, 'class_report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4e4ab",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc4343",
   "metadata": {},
   "source": [
    "Here's a plot of the errors made by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f265b",
   "metadata": {},
   "source": [
    "## Precision versus recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac5874",
   "metadata": {},
   "source": [
    "We remember the so-called \"bias-variance-tradeoff\" and that there is typically a tradeoff between precision and recall. We can visualize where our models have set the thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, 'threshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e08e4",
   "metadata": {},
   "source": [
    "If you want to change this threshold (e.g., if false positives are worse than false negatives in your specific case), you can use the method `optimize_threshold`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc718b3f",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfd331",
   "metadata": {},
   "source": [
    "Which features does the model lean on the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c223394",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_model, 'feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c8aa4-e7d6-4642-9af0-b6d39210becb",
   "metadata": {},
   "source": [
    "Note that this can vary between different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_models[1], 'feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a76ca3-2c69-43e7-85a6-9ff1bc97b909",
   "metadata": {},
   "source": [
    "## ExplainerDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58fcf3-f95d-454f-bbf5-e536f9248fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard(best_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623183a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Use the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334ec13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict on the test data we put aside earlier\n",
    "y_pred = predict_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ada32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e48fb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Export the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf69ed",
   "metadata": {},
   "source": [
    "When you are done constructing, training, evaluating, and interpreting the models, it's time to deploy them. First, you'll want to export the model together with the entire pipeline for pre-processing to, for example, the hard drive, to memory, or to or cloud provider.\n",
    "\n",
    "When you start making predictions on entirely new data (in other words, after you've completed the first stage of the model building), then you can use `predict_model` on this data. The data will then be preprocessed according to the pipeline and passed through the model.\n",
    "\n",
    "Remark: until now, we've put aside some data for testing. If you're done constructing the model, there's no point in not using this (often valuable) labeled data for training. One would like to train the model on _all_ the available labeled data. \n",
    "\n",
    "This can be achieved by using `finalize_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9281ae",
   "metadata": {},
   "source": [
    "Then we can save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef62351",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(final_model,'saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c5591",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80332b62",
   "metadata": {},
   "source": [
    "PyCaret has built-in functionality for deployment to AWS, GCP, and Azure: https://pycaret.gitbook.io/docs/get-started/functions/deploy. But you're, of course, free to deploy anywhere else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb211c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?deploy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20cbcf3-654e-463d-abb7-401954fde062",
   "metadata": {},
   "source": [
    "We can also create a simple POST API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15ca23-ad67-4e57-ae4b-e4ed72de3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_api(best_model[0], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0af96e-7ade-428c-8fc8-086ebd1cc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e853b9-2844-436e-8b2b-92ed6b1d3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0dcd0-b76f-461e-856e-4d3ba0bd7838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret",
   "language": "python",
   "name": "pycaret"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
